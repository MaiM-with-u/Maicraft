#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLMå“åº”é€Ÿåº¦æµ‹è¯•è„šæœ¬
æµ‹è¯•å¤šä¸ªä¸åŒVLMçš„å“åº”æ—¶é—´å’Œæ€§èƒ½
"""

import asyncio
import time
import json
import statistics
from datetime import datetime
from typing import Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from openai_client.modelconfig import ModelConfig
from openai_client.llm_request import LLMClient
from vlm_test_config import VLM_CONFIGS, TEST_IMAGES, TEST_PROMPTS, TEST_PARAMS


class VLMSpeedTester:
    """VLMå“åº”é€Ÿåº¦æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.log_file = f"vlm_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
    async def test_single_vlm(
        self, 
        vlm_name: str, 
        config: Dict[str, Any], 
        image_url: str, 
        prompt: str
    ) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªVLMçš„å“åº”é€Ÿåº¦"""
        
        try:
            # åˆ›å»ºæ¨¡å‹é…ç½®
            model_config = ModelConfig(
                model_name=config["model_name"],
                api_key=config["api_key"],
                base_url=config["base_url"],
                max_tokens=config["max_tokens"],
                temperature=config["temperature"]
            )
            
            # åˆ›å»ºLLMå®¢æˆ·ç«¯
            client = LLMClient(model_config)
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # å‘é€VLMè¯·æ±‚
            response = await client.vision_completion(
                prompt=prompt,
                images=image_url,
                system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒåˆ†æåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
            )
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = end_time - start_time
            
            result = {
                "vlm_name": vlm_name,
                "model_name": config["model_name"],
                "prompt": prompt,
                "image_url": image_url,
                "response_time": response_time,
                "success": response.get("success", False),
                "content_length": len(response.get("content", "")) if response.get("content") else 0,
                "token_usage": response.get("usage", {}),
                "error": response.get("error") if not response.get("success") else None,
                "timestamp": datetime.now().isoformat(),
                # æ·»åŠ æ¨¡å‹å›å¤å†…å®¹è®°å½•
                "model_response": response.get("content", "") if response.get("success") else None,
                "model_name_used": response.get("model", config["model_name"]) if response.get("success") else None,
                "finish_reason": response.get("finish_reason", None) if response.get("success") else None,
                # æ·»åŠ æ¨ç†é“¾è®°å½•
                "reasoning_content": response.get("reasoning_content", None) if response.get("success") else None
            }
            
            return result
            
        except Exception as e:
            return {
                "vlm_name": vlm_name,
                "model_name": config.get("model_name", "unknown"),
                "prompt": prompt,
                "image_url": image_url,
                "response_time": None,
                "success": False,
                "content_length": 0,
                "token_usage": {},
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                # æ·»åŠ æ¨¡å‹å›å¤å†…å®¹è®°å½•ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
                "model_response": None,
                "model_name_used": None,
                "finish_reason": None,
                # æ·»åŠ æ¨ç†é“¾è®°å½•ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
                "reasoning_content": None
            }
    
    async def test_vlm_multiple_runs(
        self, 
        vlm_name: str, 
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å¯¹å•ä¸ªVLMè¿›è¡Œå¤šæ¬¡æµ‹è¯•"""
        
        print(f"\nå¼€å§‹æµ‹è¯• {vlm_name} ({config['model_name']})...")
        
        all_results = []
        successful_runs = 0
        
        for run in range(TEST_PARAMS["num_runs"]):
            print(f"  ç¬¬ {run + 1} æ¬¡æµ‹è¯•...")
            
            # éšæœºé€‰æ‹©å›¾ç‰‡å’Œæç¤ºè¯
            import random
            image_url = random.choice(TEST_IMAGES)
            prompt = random.choice(TEST_PROMPTS)
            
            # æ‰§è¡Œæµ‹è¯•ï¼ˆä¸æ‰“å°è¯¦ç»†å‚æ•°ï¼‰
            result = await self.test_single_vlm(vlm_name, config, image_url, prompt)
            all_results.append(result)
            
            if result["success"]:
                successful_runs += 1
                # æ˜¾ç¤ºæ¨¡å‹å›å¤çš„æ‘˜è¦ä¿¡æ¯
                response_content = result.get("model_response", "")
                if response_content:
                    # æˆªå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
                    summary = response_content[:100] + "..." if len(response_content) > 100 else response_content
                    print(f"    âœ“ æˆåŠŸ - å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
                    print(f"      å›å¤æ‘˜è¦: {summary}")
                else:
                    print(f"    âœ“ æˆåŠŸ - å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
            else:
                print(f"    âœ— å¤±è´¥ - é”™è¯¯: {result['error']}")
            
            # æµ‹è¯•é—´éš”
            if run < TEST_PARAMS["num_runs"] - 1:
                await asyncio.sleep(TEST_PARAMS["delay_between_tests"])
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        successful_times = [r["response_time"] for r in all_results if r["success"] and r["response_time"] is not None]
        
        stats = {
            "vlm_name": vlm_name,
            "model_name": config["model_name"],
            "total_runs": TEST_PARAMS["num_runs"],
            "successful_runs": successful_runs,
            "success_rate": successful_runs / TEST_PARAMS["num_runs"] if TEST_PARAMS["num_runs"] > 0 else 0,
            "response_times": successful_times,
            "avg_response_time": statistics.mean(successful_times) if successful_times else None,
            "min_response_time": min(successful_times) if successful_times else None,
            "max_response_time": max(successful_times) if successful_times else None,
            "std_response_time": statistics.stdev(successful_times) if len(successful_times) > 1 else None,
            "all_results": all_results
        }
        
        return stats
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰VLMæµ‹è¯•"""
        
        print("ğŸš€ å¼€å§‹VLMå“åº”é€Ÿåº¦æµ‹è¯•")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•é…ç½®: {TEST_PARAMS['num_runs']} æ¬¡è¿è¡Œ")
        print("=" * 60)
        
        all_stats = {}
        
        # é€ä¸ªæµ‹è¯•æ¯ä¸ªVLM
        for vlm_name, config in VLM_CONFIGS.items():
            try:
                stats = await self.test_vlm_multiple_runs(vlm_name, config)
                all_stats[vlm_name] = stats
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                if stats["successful_runs"] > 0:
                    print(f"  ğŸ“Š {vlm_name} ç»Ÿè®¡:")
                    print(f"    æˆåŠŸç‡: {stats['success_rate']:.1%}")
                    print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ç§’")
                    print(f"    æœ€å¿«å“åº”: {stats['min_response_time']:.2f}ç§’")
                    print(f"    æœ€æ…¢å“åº”: {stats['max_response_time']:.2f}ç§’")
                else:
                    print(f"  âŒ {vlm_name} æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
                
            except Exception as e:
                print(f"  ğŸ’¥ æµ‹è¯• {vlm_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                all_stats[vlm_name] = {
                    "vlm_name": vlm_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_stats)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(all_stats)
        
        return all_stats
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {self.log_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    def generate_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ VLMå“åº”é€Ÿåº¦æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        # è¿‡æ»¤å‡ºæˆåŠŸçš„æµ‹è¯•ç»“æœ
        successful_tests = {
            name: stats for name, stats in results.items() 
            if isinstance(stats, dict) and stats.get("successful_runs", 0) > 0
        }
        
        if not successful_tests:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return
        
        # æŒ‰å¹³å‡å“åº”æ—¶é—´æ’åº
        sorted_tests = sorted(
            successful_tests.items(),
            key=lambda x: x[1].get("avg_response_time", float('inf'))
        )
        
        print("\nğŸ† æ€§èƒ½æ’å (æŒ‰å¹³å‡å“åº”æ—¶é—´æ’åº):")
        print("-" * 60)
        
        for rank, (vlm_name, stats) in enumerate(sorted_tests, 1):
            avg_time = stats.get("avg_response_time", 0)
            success_rate = stats.get("success_rate", 0)
            model_name = stats.get("model_name", "unknown")
            
            print(f"{rank:2d}. {vlm_name:20s} | {avg_time:6.2f}ç§’ | æˆåŠŸç‡: {success_rate:5.1%} | {model_name}")
        
        # æ€»ä½“ç»Ÿè®¡
        all_times = []
        total_successful = 0
        total_runs = 0
        
        for stats in successful_tests.values():
            times = stats.get("response_times", [])
            all_times.extend(times)
            total_successful += stats.get("successful_runs", 0)
            total_runs += stats.get("total_runs", 0)
        
        if all_times:
            print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            print(f"   æ€»æµ‹è¯•æ¬¡æ•°: {total_runs}")
            print(f"   æˆåŠŸæ¬¡æ•°: {total_successful}")
            print(f"   æ€»ä½“æˆåŠŸç‡: {total_successful/total_runs:.1%}")
            print(f"   æœ€å¿«å“åº”: {min(all_times):.2f}ç§’")
            print(f"   æœ€æ…¢å“åº”: {max(all_times):.2f}ç§’")
            print(f"   å¹³å‡å“åº”: {statistics.mean(all_times):.2f}ç§’")
        
        # æ·»åŠ æ¨¡å‹å›å¤å†…å®¹ç»Ÿè®¡
        print("\nğŸ’¬ æ¨¡å‹å›å¤å†…å®¹ç»Ÿè®¡:")
        print("-" * 60)
        
        for vlm_name, stats in successful_tests.items():
            all_results = stats.get("all_results", [])
            successful_results = [r for r in all_results if r.get("success")]
            
            if successful_results:
                # è®¡ç®—å¹³å‡å›å¤é•¿åº¦
                response_lengths = [len(r.get("model_response", "")) for r in successful_results if r.get("model_response")]
                avg_length = statistics.mean(response_lengths) if response_lengths else 0
                
                # ç»Ÿè®¡å›å¤å®ŒæˆåŸå› 
                finish_reasons = {}
                for r in successful_results:
                    reason = r.get("finish_reason", "unknown")
                    finish_reasons[reason] = finish_reasons.get(reason, 0) + 1
                
                print(f"  {vlm_name}:")
                print(f"    å¹³å‡å›å¤é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                print(f"    å›å¤å®ŒæˆåŸå› : {finish_reasons}")
                
                # æ·»åŠ æ¨ç†é“¾ç»Ÿè®¡
                reasoning_results = [r for r in successful_results if r.get("reasoning_content")]
                if reasoning_results:
                    print(f"    æ¨ç†é“¾å¯ç”¨: æ˜¯ ({len(reasoning_results)}/{len(successful_results)})")
                    # è®¡ç®—å¹³å‡æ¨ç†é“¾é•¿åº¦
                    reasoning_lengths = [len(r.get("reasoning_content", "")) for r in reasoning_results if r.get("reasoning_content")]
                    if reasoning_lengths:
                        avg_reasoning_length = statistics.mean(reasoning_lengths)
                        print(f"    å¹³å‡æ¨ç†é“¾é•¿åº¦: {avg_reasoning_length:.0f} å­—ç¬¦")
                else:
                    print("    æ¨ç†é“¾å¯ç”¨: å¦")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {self.log_file}")
        
        # å®Œæ•´è¾“å‡ºä¸åŒæ¨¡å‹çš„å›ç­”å†…å®¹
        print("\nğŸ” å®Œæ•´æ¨¡å‹å›ç­”å†…å®¹å¯¹æ¯”:")
        print("=" * 80)
        
        for vlm_name, stats in successful_tests.items():
            all_results = stats.get("all_results", [])
            successful_results = [r for r in all_results if r.get("success")]
            
            if successful_results:
                print(f"\nğŸ“ {vlm_name} ({stats.get('model_name', 'unknown')}) å®Œæ•´å›ç­”:")
                print("-" * 60)
                
                for i, result in enumerate(successful_results, 1):
                    prompt = result.get("prompt", "")
                    response = result.get("model_response", "")
                    reasoning = result.get("reasoning_content", "")
                    response_time = result.get("response_time", 0)
                    
                    print(f"\nç¬¬ {i} æ¬¡æµ‹è¯•:")
                    print(f"æç¤ºè¯: {prompt}")
                    print(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                    
                    # æ˜¾ç¤ºæ¨ç†é“¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    if reasoning:
                        print("æ¨ç†é“¾:")
                        print(f"{reasoning}")
                        print("æœ€ç»ˆå›ç­”:")
                    else:
                        print("å›ç­”å†…å®¹:")
                    
                    print(f"{response}")
                    print("-" * 40)


async def main():
    """ä¸»å‡½æ•°"""
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥
    print("ğŸ” æ£€æŸ¥é…ç½®æ–‡ä»¶...")
    missing_keys = []
    
    for vlm_name, config in VLM_CONFIGS.items():
        if config["api_key"].startswith("your_") or config["api_key"] == "":
            missing_keys.append(vlm_name)
    
    if missing_keys:
        print("âš ï¸  ä»¥ä¸‹VLMç¼ºå°‘æœ‰æ•ˆçš„APIå¯†é’¥:")
        for name in missing_keys:
            print(f"   - {name}")
        print("\nè¯·åœ¨ vlm_test_config.py ä¸­é…ç½®æœ‰æ•ˆçš„APIå¯†é’¥åå†è¿è¡Œæµ‹è¯•ã€‚")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
    tester = VLMSpeedTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
